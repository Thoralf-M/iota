// Copyright (c) Mysten Labs, Inc.
// Modifications Copyright (c) 2025 IOTA Stiftung
// SPDX-License-Identifier: Apache-2.0

use std::{collections::HashMap, sync::Arc};

use async_trait::async_trait;
use backoff::{ExponentialBackoff, backoff::Backoff};
use futures::StreamExt;
use iota_types::messages_checkpoint::CheckpointSequenceNumber;
use tokio::sync::mpsc;
use tokio_stream::wrappers::ReceiverStream;
use tokio_util::sync::CancellationToken;

use crate::{
    IngestionError, IngestionResult, MAX_CHECKPOINTS_IN_PROGRESS, Worker, util::reset_backoff,
    worker_pool::WorkerPoolStatus,
};

/// Represents the outcome of a commit operation with retry logic.
pub enum CommitStatus {
    /// Commit succeeded, continue processing.
    Success,
    /// Processing should stop due to shutdown signal.
    Shutdown,
}

/// Processes and commits batches of messages produced by workers.
///
/// The Reducer trait provides batch processing capabilities for messages
/// generated by [`Worker`]s. It allows for custom batching logic and efficient
/// processing of worker results.
///
/// # Batch Processing
///
/// [`Messages`](Worker::Message) are accumulated in batches based on the
/// [`should_close_batch`](Reducer::should_close_batch) policy.
///
/// When a batch is
/// ready to be committed (as determined by `should_close_batch`), the
/// [`commit`](Reducer::commit) method is called with the collected messages.
#[async_trait]
pub trait Reducer<Mapper: Worker>: Send + Sync {
    /// Commits a batch of messages.
    ///
    /// This method is called when a batch is ready to be processed, as
    /// determined by [`should_close_batch`](Reducer::should_close_batch). The
    /// implementation should handle the batch processing logic and return
    /// an error if the operation fails.
    ///
    /// # Note
    /// Messages within each batch are guaranteed to be ordered by checkpoint
    /// sequence number.
    async fn commit(&self, batch: &[Mapper::Message]) -> Result<(), Mapper::Error>;

    /// Attempts to commit a batch of messages with exponential backoff retries
    /// on failure.
    ///
    /// This function repeatedly calls the [`commit`](Reducer::commit) method of
    /// the provided [`Reducer`] until either:
    /// - The commit succeeds, returning `CommitStatus::Success`.
    /// - A cancellation signal is received via the [`CancellationToken`],
    ///   returning `CommitStatus::Shutdown`.
    /// - All retry attempts are exhausted within backoff's maximum elapsed
    ///   time, causing a panic.
    ///
    /// # Retry Mechanism:
    /// - Uses [`ExponentialBackoff`](backoff::ExponentialBackoff) to introduce
    ///   increasing delays between retry attempts.
    /// - Checks for cancellation both before and after each commit attempt.
    /// - If a cancellation signal is received during a backoff delay, the
    ///   function exits immediately with `CommitStatus::Shutdown`.
    ///
    /// # Panics:
    /// - If all retry attempts are exhausted within backoff's the maximum
    ///   elapsed time, indicating a persistent failure.
    async fn commit_with_retry(
        &self,
        batch: &[Mapper::Message],
        mut backoff: ExponentialBackoff,
        token: &CancellationToken,
    ) -> CommitStatus {
        loop {
            // check for cancellation before attempting commit.
            if token.is_cancelled() {
                return CommitStatus::Shutdown;
            }
            // attempt to commit.
            match self.commit(batch).await {
                Ok(_) => return CommitStatus::Success,
                Err(err) => {
                    let err = IngestionError::Reducer(format!("failed to commit batch: {err}"));
                    tracing::warn!("transient reducer commit error {err:?}");
                    // check for cancellation after failed commit.
                    if token.is_cancelled() {
                        return CommitStatus::Shutdown;
                    }
                }
            }
            // get next backoff duration or panic if max retries exceeded
            let duration = backoff
                .next_backoff()
                .expect("max retry attempts exceeded: commit operation failed");
            // if cancellation occurs during backoff wait, exit early with Shutdown.
            // Otherwise (if timeout expires), continue with the next retry attempt
            if tokio::time::timeout(duration, token.cancelled())
                .await
                .is_ok()
            {
                return CommitStatus::Shutdown;
            }
        }
    }

    /// Determines if the current batch should be closed and committed.
    ///
    /// This method is called for each new message to determine if the current
    /// batch should be committed before adding the new message.
    ///
    /// # Default Implementation
    ///
    /// By default, returns `true` only when there are no more messages
    /// (`next_item` is `None`), effectively creating a single batch for all
    /// messages.
    fn should_close_batch(
        &self,
        _batch: &[Mapper::Message],
        next_item: Option<&Mapper::Message>,
    ) -> bool {
        next_item.is_none()
    }
}

/// Processes worker messages and manages batching through a reducer.
///
/// This function is the core of the reduction pipeline, handling message
/// batching, watermark tracking, and progress reporting. It maintains message
/// order by checkpoint sequence number and applies batching logic through the
/// provided reducer.
///
/// # Message Processing Flow
///
/// 1. Receives messages in chunks up to [`MAX_CHECKPOINTS_IN_PROGRESS`].
/// 2. Maintains message order using checkpoint sequence numbers.
/// 3. Batches messages according to reducer's [`Reducer::should_close_batch`]
///    policy and after that its committed.
/// 4. Progress is updated after each commit.
/// 5. Reports progress back to the executor.
///
/// # Shutdown Behavior
///
/// The function will gracefully exit when receiving a shutdown signal,
/// ensuring no data loss for processed messages.
pub(crate) async fn reduce<W: Worker>(
    task_name: String,
    mut current_checkpoint_number: CheckpointSequenceNumber,
    watermark_receiver: mpsc::Receiver<(CheckpointSequenceNumber, W::Message)>,
    executor_progress_sender: mpsc::Sender<WorkerPoolStatus>,
    reducer: Box<dyn Reducer<W>>,
    backoff: Arc<ExponentialBackoff>,
    token: CancellationToken,
) -> IngestionResult<()> {
    // convert to a stream of MAX_CHECKPOINTS_IN_PROGRESS size. This way, each
    // iteration of the loop will process all ready messages.
    let mut stream =
        ReceiverStream::new(watermark_receiver).ready_chunks(MAX_CHECKPOINTS_IN_PROGRESS);
    // store unprocessed progress messages from workers.
    let mut unprocessed = HashMap::new();
    // buffer to accumulate results before passing them to the reducer.
    // The size of this batch is dynamically determined by the reducer's
    // `should_close_batch` method.
    let mut batch = vec![];
    // track the next unprocessed checkpoint number for reporting progress
    // after each chunk of messages is received from the stream.
    let mut progress_update = None;
    // flag to indicate a shutdown has been triggered.
    let mut trigger_shutdown = false;

    while let Some(update_batch) = stream.next().await {
        unprocessed.extend(update_batch.into_iter());
        // Process messages sequentially based on checkpoint sequence number.
        // This ensures in-order processing and maintains progress integrity.
        while let Some(message) = unprocessed.remove(&current_checkpoint_number) {
            // reducer is configured, collect messages in batch based on
            // `reducer.should_close_batch` policy, once a batch is collected it gets
            // committed and a new batch is created with the current message.
            if reducer.should_close_batch(&batch, Some(&message)) {
                match reducer
                    .commit_with_retry(&std::mem::take(&mut batch), reset_backoff(&backoff), &token)
                    .await
                {
                    CommitStatus::Success => {
                        batch = vec![message];
                        progress_update = Some(current_checkpoint_number);
                    }
                    CommitStatus::Shutdown => {
                        trigger_shutdown = true;
                        break;
                    }
                };
            } else {
                // Add message to existing batch since no commit needed.
                batch.push(message);
            }
            current_checkpoint_number += 1;
        }
        // Handle final batch processing.
        // Check if the final batch should be committed.
        // None parameter indicates no more messages available.
        if reducer.should_close_batch(&batch, None) && !trigger_shutdown {
            match reducer
                .commit_with_retry(&std::mem::take(&mut batch), reset_backoff(&backoff), &token)
                .await
            {
                CommitStatus::Success => {
                    progress_update = Some(current_checkpoint_number);
                }
                CommitStatus::Shutdown => trigger_shutdown = true,
            }
        }
        // report progress update to executor.
        if let Some(watermark) = progress_update {
            executor_progress_sender
                .send(WorkerPoolStatus::Running((task_name.clone(), watermark)))
                .await
                .map_err(|_| IngestionError::Channel("unable to send worker pool progress updates to executor, receiver half closed".into()))?;
            progress_update = None;
        }

        // Check for shutdown signal after progress update to ensure progress
        // is reported even during shutdown.
        if trigger_shutdown {
            break;
        }
    }
    Ok(())
}
